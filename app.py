from flask import Flask, request, jsonify
from transformers import pipeline, set_seed
import torch

app = Flask(__name__)

# --- AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ ---
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€ã‚ˆã‚Šè»½é‡ãª 'xsmall' ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚
try:
    print("ğŸ”„ Loading AI Model: rinna/japanese-gpt2-xsmall...")
    generator = pipeline(
        'text-generation',
        # â–¼â–¼â–¼ ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ã®ãŸã‚ã®æœ€é‡è¦å¤‰æ›´ç‚¹ â–¼â–¼â–¼
        model='rinna/japanese-gpt2-xsmall', 
        # â–²â–²â–² rinna/japanese-gpt2-small ã‹ã‚‰å¤‰æ›´ã—ã¾ã—ãŸ â–²â–²â–²
        device=-1, # CPUã‚’ä½¿ç”¨
        # CPUå®Ÿè¡Œã®å ´åˆã€float16ã¯é€Ÿåº¦ä½ä¸‹ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’æ¨å¥¨
        # torch_dtype=torch.float16 
    )
    print("âœ… AI Model loaded successfully.")
except Exception as e:
    print(f"âŒ Failed to load AI model: {e}")
    generator = None

@app.route("/")
def health():
    """ã‚µãƒ¼ãƒãƒ¼ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    status = "ok" if generator else "error: model not loaded"
    return {"status": status}

@app.route("/generate", methods=["GET"])
def generate_text():
    """ãƒ–ãƒ­ã‚°ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æŠœç²‹ã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    if not generator:
        return jsonify({"error": "AI model is not available"}), 503

    # URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æŠœç²‹ã‚’å–å¾—
    title = request.args.get("title", "").strip()
    excerpt = request.args.get("excerpt", "").strip()

    # titleãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å¿…é ˆ
    if not title:
        return jsonify({"error": "title is a required parameter"}), 400

    # AIã«å…¥åŠ›ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæŒ‡ç¤ºæ–‡ï¼‰ã‚’ä½œæˆ
    prompt = f"""
ä»¥ä¸‹ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æŠœç²‹ã‚’å…ƒã«ã€èª­è€…ã®èˆˆå‘³ã‚’å¼•ãã‚ˆã†ãªé­…åŠ›çš„ãªãƒ„ã‚¤ãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:
{title}

# è¨˜äº‹ã®æŠœç²‹:
{excerpt}

# ç”Ÿæˆãƒ„ã‚¤ãƒ¼ãƒˆ:
"""

    try:
        # æ¯å›é•ã†çµæœã‚’å‡ºã™ãŸã‚ã«ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š
        set_seed(torch.randint(0, 10000, (1,)).item())
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å®Ÿè¡Œ
        generated_outputs = generator(
            prompt,
            max_length=150,              # ç”Ÿæˆã™ã‚‹æ–‡ç« ã®æœ€å¤§é•·
            num_return_sequences=1,      # ç”Ÿæˆã™ã‚‹æ–‡ç« ã®æ•°
            do_sample=True,              # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
            top_k=50,                    # ä¸Šä½kå€‹ã®å˜èªã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            top_p=0.95,                  # ä¸Šä½p%ã®å˜èªã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            temperature=0.9,             # ç”Ÿæˆã®å¤šæ§˜æ€§ã‚’èª¿æ•´ (é«˜ã„ã»ã©å¤šæ§˜)
            no_repeat_ngram_size=2       # åŒã˜2å˜èªã®ç¹°ã‚Šè¿”ã—ã‚’é˜²ã
        )
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰ã€å¿…è¦ãªéƒ¨åˆ†ã ã‘ã‚’æŠ½å‡º
        generated_text = generated_outputs[0]['generated_text']
        # "# ç”Ÿæˆãƒ„ã‚¤ãƒ¼ãƒˆ:" ã®å¾Œã‚ã®éƒ¨åˆ†ã‚’å–ã‚Šå‡ºã™
        tweet_text = generated_text.split("# ç”Ÿæˆãƒ„ã‚¤ãƒ¼ãƒˆ:")[1].strip()

        # çµæœã‚’JSONå½¢å¼ã§è¿”ã™
        return jsonify({"generated_text": tweet_text})

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        return jsonify({"error": f"AI generation failed: {str(e)}"}), 500

# ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
if __name__ == "__main__":
    # host="0.0.0.0" ã§å¤–éƒ¨ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’å—ã‘ä»˜ã‘ã‚‹
    app.run(host="0.0.0.0", port=8000)
